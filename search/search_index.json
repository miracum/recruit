{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":"<p>recruIT is a software to assist with the recruitment of patients for clinical trials.</p> <p>It regularly queries a patient health data repository for potential candidates and displays them on a list to support manual screening.</p> <p>It supports querying OMOP CDM databases and eligibility criteria can be defined using OHDSI Atlas.</p> <p>Tip</p> <p>Don't have or want to use the OHDSI stack? Check out the new way to run recruIT directly on tabular FHIR data here.</p> <p>Internally, the modules communicate using HL7 FHIR\u00ae via a central FHIR server.</p> <p>Get Started \ud83d\ude80</p>"},{"location":"#architecture","title":"Architecture","text":"<p>Fundamentally, the design of the application is based on the publication Design and Multicentric Implementation of a Generic Software Architecture for Patient Recruitment Systems Re-Using Existing HIS Tools and Routine Patient Data by Trinczek et al. Of the 5 presented modules, recruIT implements the notification module (<code>notify</code>), the screening list module (<code>list</code>), and the query module (<code>query</code>). The patient data module is based on the OMOP CDM, OHDSI Atlas is used to define trial metadata.</p> <p></p>"},{"location":"#query-module","title":"Query Module","text":"<p>The query module, also referred to as <code>query</code>, uses the OHDSI WebAPI to list the studies (\"cohort definitions\") created using OHDSI Atlas, generates them to identify the patients in the OMOP database, maps these OMOP patients to FHIR resources, and finally sends them to a FHIR server for persistence.</p>"},{"location":"#list-module","title":"List Module","text":"<p>The list module (<code>list</code>), reads the list of potentially eligible patients from the FHIR server and displays them to the user. It displays information including the patient identifier, year of birth, and the physical location of the last known stay. Users can interact with this list by changing the recruitment status for each recommendation and viewing a basic, pseudonymized patient record.</p>"},{"location":"#notification-module","title":"Notification Module","text":"<p>The notification module (<code>notify</code>) is used to send email notifications to users whenever a screening list is updated with new candidates. It uses the subscription mechanism of the FHIR server to be updated about additions made by the query module.</p>"},{"location":"configuration/de-pseudonymization/","title":"De-Pseudonymization","text":"<p>Requires at least version 2.1.0 of the MIRACUM FHIR Pseudonymizer to be deployed.</p> <p>If the <code>Patient</code> and <code>Encounter</code> resources in the FHIR server are pseudonymized (which is true if the OMOP DB is pseudonymized), you may want to de-pseudonymize them before displaying them in the screening list to show the original patient and encounter identifiers.</p> <p>On the <code>list</code> module, you need to set the environment variable <code>DE_PSEUDONYMIZATION_ENABLED</code> to <code>true</code>. This will cause all resources fetched from the FHIR server to be sent to the <code>$de-pseudonymize</code> endpoint of the FHIR Pseudonymizer first, which reverts any pseudonymization or encryption previously applied to the resources. <code>DE_PSEUDONYMIZATION_SERVICE_URL</code> must be set to the base URL of this service. Because this endpoint is protected by a basic API key, you will need to configure the same key in the pseudonymizer's <code>APIKEY</code> and the list module's <code>DE_PSEUDONYMIZATION_API_KEY</code>.</p>"},{"location":"configuration/existing-fhir-server/","title":"Using an existing FHIR server","text":""},{"location":"configuration/existing-fhir-server/#fhir-subscription-support","title":"FHIR subscription support","text":"<p>The notification module relies on the server's FHIR Subscription support to be notified whenever a screening list has been updated by the query module. Many FHIR servers don't support this FHIR subscription mechanism, but there is no technical reason for the recruIT infrastructure to entirely depend on it. Please create an issue if support for a non-HAPI FHIR server is desired.</p>"},{"location":"configuration/existing-fhir-server/#pre-filled-server","title":"Pre-filled server","text":"<p>By default, the recruIT tool only needs clinical data from OMOP and Atlas and automatically creates FHIR Patient, Encounter, ResearchStudy, and ResearchSubject resources in the FHIR server to populate the screening list.</p> <p>If you have an existing FHIR server already filled with at least Patient and Encounter resources, you can use it to display additional information in the last known stay column of the screening list (contact details and a more detailed description of the last known stay location if the Encounter provides it).</p> <p>Further, if the existing server contains Condition, Procedure, Observation, and MedicationStatement resources, a small integrated patient record is populated.</p> <p>To make sure the existing patient resources are correctly referenced within the screening list, you have to configure the patient identifier FHIR system in the query module. Add the following environment variable to the <code>query</code> service:</p> <p><code>FHIR_SYSTEMS_PATIENT_ID: https://fhir.miracum.org/core/NamingSystem/patientId</code></p> <p>replacing <code>https://fhir.miracum.org/core/NamingSystem/patientId</code> with the MRN identifier used by the Patient resources already existing in the server.</p> <p>You can prevent the query module from creating Encounter resources itself by setting:</p> <p><code>QUERY_EXCLUDE_PATIENT_PARAMETERS_ENCOUNTER: true</code></p> <p>and prevent it from overriding existing Patient resources by setting:</p> <p><code>QUERY_ONLY_CREATE_PATIENTS_IF_NOT_EXIST: true</code></p> <p>Race condition on concurrent writes to the FHIR server</p> <p>There is potential for a race condition on the Patient resources created by the query module and the existing ones if the latter are continuously and possibly asynchronously transmitted to the server: the query module maps OMOP's <code>person</code> table to Patient resources and sends them to the FHIR server as conditional updates on the patient identifier. If a Patient resource with the same identifier already exists, its contents are simply overwritten.</p> <p>If the query module is able to generate a Patient with an identifier that doesn't yet exist in the FHIR server and the other mechanism to send existing resources to the server is using \"upsert\" or \"update-as-create\" semantics to send a Patient after the fact, then two Patient resources with the same identifier may exist in the server. This will cause a conflict the next time the query module tries to update the Patient and can only be resolved by deleting the resource first created by the query module.</p> <p>A solution would be to ensure that all Patient resources exist in the FHIR server before the query module is ran. Or the existing mechanism to send resources should be switched to using conditional-updates/conditional-creates.</p>"},{"location":"configuration/options/","title":"Configuration Options","text":"<p>This is a non-exhaustive list of available configuration options for the recruIT deployment.</p> <p>When deploying using Helm, many of these options are automatically configured or exposed via <code>values.yaml</code>. You can find a complete description of all available chart configuration options here: https://github.com/miracum/charts/blob/master/charts/recruit/README.md#configuration. If anything is missing, you can use the <code>extraEnv</code> option to supply additional environment variables to the modules.</p> <p>\"Staging Default Value\" refers to the default value for that option when deploying via Docker Compose using the <code>.staging.env</code> file.</p>"},{"location":"configuration/options/#used-by-multiple-modules","title":"Used by multiple modules","text":"Variable Description Staging Default Value FHIR_URL URL of a FHIR server used to store the screening lists and retrieve patient data (if available). Used by all modules. <code>http://fhir:8080/fhir</code>"},{"location":"configuration/options/#screening-list","title":"Screening List","text":"Variable Description Staging Default Value KEYCLOAK_DISABLED Disable Keycloak authentication for the screening list. <code>false</code> KEYCLOAK_CLIENT_ID Keycloak client id for the screening list component. <code>uc1-screeninglist</code> KEYCLOAK_REALM The Keycloak realm. <code>MIRACUM</code> KEYCLOAK_AUTH_URL URL of the Keycloak server (should end with <code>/auth</code>). <code>http://host.docker.internal:38086/auth</code> DE_PSEUDONYMIZATION_ENABLED Whether or not the resources from the FHIR server should be de-pseudonymized before being displayed in the screening list. See De-Pseudonymization for details <code>false</code> DE_PSEUDONYMIZATION_SERVICE_URL The URL to the FHIR Pseudonymizer service used for de-pseudonymization. <code>\"\"</code> DE_PSEUDONYMIZATION_API_KEY The API key used to authenticate against the FHIR Pseudonymizer <code>\"\"</code> HIDE_DEMOGRAPHICS Don't show age and gender of the persons <code>false</code> HIDE_LAST_VISIT Don't show the last visit information <code>false</code> HIDE_EHR_BUTTON Don't show the button to show EHR information of the person <code>false</code> PROXY_IS_SECURE_BACKEND If <code>FHIR_URL</code> points to a server using HTTPS, then you should set this to <code>1</code> or <code>true</code> <code>false</code>"},{"location":"configuration/options/#query-module","title":"Query Module","text":"Variable Description Staging Default Value ATLAS_URL URL of the ATLAS WebAPI endpoint. Usually ends in <code>/WebAPI</code>. Used by the query module.\u00b9 <code>http://ohdsi-webapi:8080/WebAPI</code> ATLAS_DATASOURCE Name of the ATLAS datasource used to generate the cohorts from. <code>OHDSI-CDMV5</code> OMOP_JDBCURL JDBC URL of the OMOP database. <code>jdbc:postgresql://omopdb:5432/OHDSI</code> OMOP_USERNAME Username to access the OMOP database. <code>postgres</code> OMOP_PASSWORD Password to access the OMOP database. <code>postgres</code> OMOP_RESULTSSCHEMA Name of the database schema containing the results of the cohort generation. <code>synpuf_results</code> OMOP_CDMSCHEMA Name of the database schema containing the actual clinical data. <code>synpuf_cdm</code> QUERY_SCHEDULE_UNIXCRON A UNIX-compliant CRON expression to configure the execution schedule of the query module (see https://crontab.guru/). <code>*/5 * * * *</code> (Run every 5 minutes) QUERY_SELECTOR_MATCHLABELS Comma-separated list of labels which must be present in either the cohort's name or description enclosed in <code>[]</code> in order to be processed by the query module.\u00b2 <code>UC1,Test</code> QUERY_WEBAPI_AUTH_ENABLED Set to true if the OHDSI WebAPI requires authentication. <code>false</code> QUERY_WEBAPI_AUTH_LOGIN_PATH The login method to use. See https://github.com/OHDSI/Atlas/blob/master/js/config/app.js#L20 for a list of possible paths. <code>/user/login/db</code> QUERY_WEBAPI_AUTH_USERNAME The username to login as. Note that this user needs permissions to query and generate cohorts. <code>\"\"</code> QUERY_WEBAPI_AUTH_PASSWORD The password used to login. <code>\"\"</code> QUERY_APPEND_RECOMMENDATIONS_TO_EXISTING_LIST if true, instead of overwriting the contents of the List for each cohort based on what the last generation run returned, append to this list <code>false</code> QUERY_FORCE_UPDATE_SCREENING_LIST if true, always send a List resource as part of the transaction even if nothing changed <code>false</code> QUERY_ONLY_CREATE_PATIENTS_IF_NOT_EXIST if true, send Patient resources as \"conditional-creates\" on their first identifier instead of using \"conditional-update\". Useful if the server is already filled with Patient resources from a different system <code>false</code> QUERY_COHORTSIZETHRESHOLD Maximum number of patients to be included in the generated screening list. A warning will appear on the screening list view if a list exceeded this count. <code>100</code> QUERY_WEBAPI_COHORT_CACHE_SCHEMA The name of the schema which contains the cohort definitions cache. <code>ohdsi</code> QUERY_RUN_ONCE_AND_EXIT If set to <code>true</code>, run the query module only once against all discovered cohort definitions and exit. Useful when running the module as part of a larger workflow. See here for an example using Argo Workflows. <code>false</code> <p>\u00b9: This is usually the same URL you configured in the <code>config-local.js</code> file when setting up the ATLAS server.</p> <p>\u00b2: For example, the default values of <code>UC1,Test</code> require the cohort definitions name or description in Atlas to contain either the string <code>[UC1]</code> or <code>[Test]</code> in order to be processed by the query module.</p>"},{"location":"configuration/options/#notification-module","title":"Notification Module","text":"Variable Description Staging Default Value NOTIFY_WEBHOOK_ENDPOINT External URL for the notification module's webhook endpoint. Should end in <code>/on-list-change</code>.\u00b3 <code>http://notify:8080/on-list-change</code> NOTIFY_RULES_CONFIG_PATH Path to the notification rule configuration file. The file is mounted inside the notification module. <code>./staging/notify-rules.yml</code> NOTIFY_MAIL_HOST Host of the SMTP server used to send notification emails. <code>maildev</code> NOTIFY_MAIL_SMTP_PORT SMTP port on the host. <code>1025</code> NOTIFY_MAIL_USERNAME SMTP username. <code>user</code> NOTIFY_MAIL_PASSWORD SMTP password. <code>pass</code> NOTIFY_MAILER_LINKTEMPLATE Template used to generate a clickable link in the notification emails. <code>[list_id]</code> is mandatory and is replaced with the lists internal id.\u2074 <code>http://recruit-list.127.0.0.1.nip.io/recommendations/[list_id]</code> NOTIFY_MAILER_FROM The sender email address for the created notification mails. <code>rekrutierung@miracum.org</code> <p>\u00b3: If your FHIR server is running on <code>fhir.example.com</code> and your notification module runs on <code>notify.example.com:8080</code>, then this value should be set to <code>http://notify.example.com:8080/on-list-change</code>. The default exposed port of the notification module is <code>38087</code>. This external port is used when the FHIR server can't access the notification module using the Docker-network internal host and port <code>notify:8080</code>.</p> <p>\u2074: If your screening list is running on <code>https://list.example.com:38080/</code>, then this value should be set to <code>https://list.example.com:38080/recommendations/[list_id]</code>.</p>"},{"location":"configuration/security/","title":"Security","text":""},{"location":"configuration/security/#authentication-and-authorization-for-the-screening-list","title":"Authentication and Authorization for the Screening List","text":"<p>Access to the screening list can configured using Keycloak as an identity provider. Since the screening list is a Single-Page Application, no client secret needs to be provided. Instead, make sure the screening list is only accessible behind TLS and the redirect URLs in Keycloak are set accordingly.</p> <p>To limit the screening lists and screening recommendations individual users can see, the notification rule configuration is used. By default, every user with a configured notification subscription is allowed to access the corresponding screening list. This ensures that when one receives an email about new screening recommendations, they can access these recommendations without extra configuration. Further, a new <code>accessibleBy.users</code> has been added to the trials. This allows specifying additional users either by email or by username which can access the trial's screening recommendations.</p> <pre><code>notify:\n  rules:\n    schedules:\n      everyMorning: \"0 0 8 1/1 * ? *\"\n      everyMonday: \"0 0 8 ? * MON *\"\n      everyHour: \"0 0 0/1 1/1 * ? *\"\n      everyFiveMinutes: \"0 0/5 * 1/1 * ? *\"\n\n    # trials are identified by their acronym which corresponds to the cohort's title in Atlas or the \"[acronym=XYZ]\" tag\n    trials:\n      # By default, every user under 'subscriptions' is also allowed to access the corresponding screening list,\n      # in the special case of '*', the user with the everything@example.com address is allowed to access every\n      # list.\n      - acronym: \"*\"\n        subscriptions:\n          - email: \"everything@example.com\"\n\n      - acronym: \"SAMPLE\"\n        # the new \"accessibleBy\" key allows specifying users either by username or email address that\n        # are allowed to access the screening list\n        accessibleBy:\n          users:\n            - \"user1\"\n            - \"user.two@example.com\"\n        subscriptions:\n          - email: \"everyMorning@example.com\"\n\n      - acronym: \"AMICA\"\n        subscriptions:\n          - email: \"everyHour1@example.com\"\n            notify: \"everyHour\"\n</code></pre> <p>Any user with the <code>admin</code> role inside the screening list client in Keycloak is allowed to access all recommendations:</p> <p></p> <p>You can disable authorization by not mounting the <code>notify-rules.yaml</code> inside the container; if no config is found, then no permissions are checked.</p>"},{"location":"configuration/security/#configuring-the-query-module-to-access-a-secured-webapi-instance","title":"Configuring the Query Module to access a secured WebAPI instance","text":"<p>If the OHDSI WebAPI requires authentication, you need to configure the query module accordingly. The relevant environment variables to set start with <code>QUERY_WEBAPI_AUTH_</code> (see the configuration overview).</p> <p>Be sure to give the created user relevant roles to access the OMOP-CDMV5 source, access cohort definitions, and generate cohorts.</p> <p>You can also combine multiple authentication methods, for example use OpenID to allow users to login via the Atlas UI but create a dedicated service account for the query module which uses WebAPI basic security.</p>"},{"location":"configuration/security/#verify-container-image-signatures-and-slsa-provenance","title":"Verify container image signatures and SLSA provenance","text":"<p>Prerequisites:</p> <ul> <li>cosign</li> <li>slsa-verifier</li> <li>crane</li> </ul> <p>All released container images are signed using cosign and SLSA Level 3 provenance is available for verification.</p> <pre><code># for example, verify the `list` module's container image. Same workflow applies to `query` and `notify`.\nIMAGE=ghcr.io/miracum/recruit/list:v10.4.4\nDIGEST=$(crane digest \"${IMAGE}\")\nIMAGE_DIGEST_PINNED=\"ghcr.io/miracum/recruit/list@${DIGEST}\"\nIMAGE_TAG=\"${IMAGE#*:}\"\n\ncosign verify \\\n   --certificate-oidc-issuer=https://token.actions.githubusercontent.com \\\n   --certificate-identity=\"https://github.com/miracum/recruit/.github/workflows/build.yaml@refs/tags/${IMAGE_TAG}\" \\\n   \"${IMAGE_DIGEST_PINNED}\"\n\nslsa-verifier verify-image \\\n    --source-uri github.com/miracum/recruit \\\n    --source-tag ${IMAGE_TAG} \\\n    \"${IMAGE_DIGEST_PINNED}\"\n</code></pre> <p>See also https://github.com/slsa-framework/slsa-github-generator/tree/main/internal/builders/container#verification for details on verifying the image integrity using automated policy controllers.</p>"},{"location":"configuration/ui-configuration/","title":"Customizing the user interface","text":""},{"location":"configuration/ui-configuration/#configuring-columns-shown-in-the-screening-list","title":"Configuring columns shown in the screening list","text":"<p>See the <code>HIDE_DEMOGRAPHICS</code>, <code>HIDE_LAST_VISIT</code>, <code>HIDE_EHR_BUTTON</code> options in the configuration section for the screening list.</p>"},{"location":"configuration/ui-configuration/#hiding-screening-lists-from-the-user-interface","title":"Hiding screening lists from the user interface","text":"<p>Requires <code>list</code> module version 2.14 or later</p> <p>Any user having the <code>admin</code> role will note switch controls to mark individual screening lists as inactive. These will then no longer show up for regular users.</p> <p></p> <p>Known bug</p> <p>Even if a list is marked as inactive, the associated clinical study still counts towards the recommendation statistic markers.</p>"},{"location":"deployment/docker-compose/","title":"Docker Compose","text":"<p>The main repository contains the <code>docker-compose.yaml</code> files to deploy recruIT via Docker Compose. The following guides assume that you have cloned it to your current working directory:</p> <pre><code>git clone https://github.com/miracum/recruit.git\ncd recruit/\n</code></pre> <p>You can also download a versioned archive of the latest <code>docker-compose.yaml</code> files from the tagged releases by visiting https://github.com/miracum/recruit/releases and getting it from the assets.</p>"},{"location":"deployment/docker-compose/#prerequisites","title":"Prerequisites","text":"<p>Docker CLI version 20.10.14 or later.</p>"},{"location":"deployment/docker-compose/#installation-for-local-testing-using-sample-data","title":"Installation for local testing using sample data","text":"<p>To run everything locally with Keycloak-based authentication enabled, a sample cohort, and an OMOP DB filled with sample data:</p> <pre><code>docker compose --project-name=recruit \\\n    --env-file=docker-compose/.staging.env \\\n    -f docker-compose/docker-compose.yaml \\\n    -f docker-compose/docker-compose.staging.yaml up\n</code></pre> <p>You can run the following to probe every component for its health status:</p> <pre><code>docker compose --project-name=recruit \\\n    --env-file=docker-compose/.staging.env \\\n    -f docker-compose/docker-compose.yaml \\\n    -f docker-compose/docker-compose.staging.yaml \\\n    -f docker-compose/docker-compose.probe.yaml run health-probes\n</code></pre> <p>The <code>docker-compose.staging.yaml</code> also includes Traefik as a reverse proxy, so you can access the services running on your local machine on the following named URLs:</p> Service Ingress URL Note OHDSI Atlas http://recruit-ohdsi.127.0.0.1.nip.io/atlas/ recruIT Screening List http://recruit-list.127.0.0.1.nip.io login with username: <code>user1</code> and password: <code>user1</code>; Or as <code>uc1-admin</code>/<code>admin</code> for full access HAPI FHIR Server http://recruit-fhir-server.127.0.0.1.nip.io MailDev http://maildev.127.0.0.1.nip.io Keycloak http://auth.127.0.0.1.nip.io/ login with username: <code>admin</code> and password: <code>admin</code> <p>By default, the query module runs every 5 minutes to check for new study candidates. After some time, you should see the following when opening the screening list at http://recruit-list.127.0.0.1.nip.io and logging in as <code>uc1-admin</code>/<code>admin</code>:</p> <p></p> <p>Clicking on the list for the <code>SAMPLE M</code> study should show the list of candidates:</p> <p></p> <p>Finally, checking the mail viewer at http://maildev.127.0.0.1.nip.io you can see the email notifications:</p> <p></p>"},{"location":"deployment/docker-compose/#stop-the-deployment","title":"Stop the deployment","text":"<p>To stop all services, run:</p> <pre><code>docker compose --project-name=recruit \\\n    --env-file=docker-compose/.staging.env \\\n    -f docker-compose/docker-compose.yaml \\\n    -f docker-compose/docker-compose.staging.yaml \\\n    down -v\n</code></pre>"},{"location":"deployment/docker-compose/#standalone-installation","title":"Standalone installation","text":"<p>The instructions above used the <code>docker-compose/docker-compose.staging.yaml</code> to deploy a FHIR server, a pre-filled OMOP CDM database, the OHDSI tools initialized with sample cohorts, a mock email viewer, and a pre-configured Keycloak.</p> <p>The recommended way to install in \"production-mode\" is to already have deployed all these services and only need to configure and deploy the recruIT modules. These modules are listed in the <code>docker-compose/docker-compose.yaml</code> file.</p> <p>Several environment variables need to be set before calling <code>docker-compose -f docker-compose/docker-compose.yaml up</code>. You can create a <code>.env</code> file in the current directory and set them according to your environment based on the <code>.staging.env</code> example configuration file in the <code>docker-compose</code> folder.</p> <p>You can find a list of available configuration options here.</p>"},{"location":"deployment/kubernetes/","title":"Kubernetes","text":""},{"location":"deployment/kubernetes/#introduction","title":"Introduction","text":"<p>A Helm chart for deploying recruIT on a Kubernetes cluster is available in the main repository's OCI registry and in the MIRACUM charts repository. The chart can be used to deploy the application as well as all dependencies required for it to run (OHDSI WebAPI, OHDSI Atlas, HAPI FHIR server). The chart also includes MailHog, a mock mail server for testing email notifications.</p> <p>Using the default values provided with the chart, all dependencies are installed and all services are configured to use them.</p>"},{"location":"deployment/kubernetes/#setup","title":"Setup","text":"<ol> <li> <p>Setup a Kubernetes cluster using your cloud provider of choice OR in a local environment using minikube,    KinD, or k3d.</p> </li> <li> <p>Install kubectl and helm</p> </li> </ol>"},{"location":"deployment/kubernetes/#installation","title":"Installation","text":"<p>Deploy recruIT to a namespace called <code>recruit</code> by running</p> <pre><code>helm install -n recruit \\\n  --create-namespace \\\n  --render-subchart-notes \\\n  --set ohdsi.cdmInitJob.enabled=true \\\n  recruit oci://ghcr.io/miracum/recruit/charts/recruit\n</code></pre> <p>As a quick check to make sure everything is running correctly, you can use the following to check the readiness of all services:</p> <pre><code>$ helm test -n recruit recruit\n\nNAME: recruit\nLAST DEPLOYED: Wed May  4 21:45:06 2022\nNAMESPACE: recruit\nSTATUS: deployed\nREVISION: 1\nTEST SUITE:     recruit-fhirserver-test-endpoints\nLast Started:   Wed May  4 22:14:23 2022\nLast Completed: Wed May  4 22:14:39 2022\nPhase:          Succeeded\nTEST SUITE:     recruit-ohdsi-test-connection\nLast Started:   Wed May  4 22:14:39 2022\nLast Completed: Wed May  4 22:14:43 2022\nPhase:          Succeeded\nTEST SUITE:     recruit-test-health-probes\nLast Started:   Wed May  4 22:14:43 2022\nLast Completed: Wed May  4 22:14:49 2022\nPhase:          Succeeded\nNOTES:\n1. Get the screening list URL by running these commands:\n  http://recruit-list.127.0.0.1.nip.io/\n</code></pre>"},{"location":"deployment/kubernetes/#example-installation-of-the-recruit-chart-with-ingress-support-using-kind","title":"Example installation of the recruIT chart with ingress support using KinD","text":"<p>This will demonstrate how to install recruIT on your local machine using KinD using the following advanced features:</p> <ul> <li>create a multi-node Kubernetes cluster to demonstrate topology-zone aware pod   spreading for high-availability deployments</li> <li>expose all user-facing services behing the NGINX ingress controller on a https://nip.io domain resolved to localhost</li> <li>enable and enforce the restricted Pod Security Standard   to demonstrate security best practices followed by all components</li> <li>pre-load the OMOP CDM database with SynPUF-based sample data</li> </ul> <p>First, create a new cluster with Ingress support:</p> <pre><code>cat &lt;&lt;EOF | kind create cluster --config=-\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nfeatureGates:\n  PodSecurity: true\nnodes:\n  - role: control-plane\n    image: docker.io/kindest/node:v1.26.0@sha256:45aa9ecb5f3800932e9e35e9a45c61324d656cf5bc5dd0d6adfc1b0f8168ec5f\n    kubeadmConfigPatches:\n      - |\n        kind: InitConfiguration\n        nodeRegistration:\n          kubeletExtraArgs:\n            node-labels: \"ingress-ready=true\"\n    extraPortMappings:\n      - containerPort: 80\n        hostPort: 80\n        protocol: TCP\n      - containerPort: 443\n        hostPort: 443\n        protocol: TCP\n    labels:\n      topology.kubernetes.io/zone: a\n  - role: worker\n    image: docker.io/kindest/node:v1.26.0@sha256:45aa9ecb5f3800932e9e35e9a45c61324d656cf5bc5dd0d6adfc1b0f8168ec5f\n    labels:\n      topology.kubernetes.io/zone: b\n  - role: worker\n    image: docker.io/kindest/node:v1.26.0@sha256:45aa9ecb5f3800932e9e35e9a45c61324d656cf5bc5dd0d6adfc1b0f8168ec5f\n    labels:\n      topology.kubernetes.io/zone: c\nEOF\n</code></pre> <p>Install the NGINX ingress controller</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.5.1/deploy/static/provider/kind/deploy.yaml\n</code></pre> <p>Wait until it's ready to process requests by running</p> <pre><code>kubectl wait --namespace ingress-nginx \\\n  --for=condition=ready pod \\\n  --selector=app.kubernetes.io/component=controller \\\n  --timeout=90s\n</code></pre> <p>Create a namespace for the new installation. Enable and enforce restricted pod security policies:</p> <pre><code>kubectl create namespace recruit\nkubectl label namespace recruit pod-security.kubernetes.io/enforce=restricted\nkubectl label namespace recruit pod-security.kubernetes.io/enforce-version=v1.26\n</code></pre> <p>Save the following as <code>values-kind-recruit.yaml</code>, or you can clone this repo and reference the file as <code>-f docs/_snippets/values-kind-recruit.yaml</code>. The <code>ohdsi.cdmInitJob.extraEnv</code> option <code>SETUP_SYNPUF=true</code> means that the OMOP database will be initialized with SynPUF 1K sample patient data.</p> <p>Documentation for all available chart options</p> <p>You can find a complete description of all available chart configuration options here: https://github.com/miracum/charts/blob/master/charts/recruit/README.md#configuration</p> values-kind-recruit.yaml<pre><code>list:\n  resources:\n    requests:\n      memory: \"128Mi\"\n      cpu: \"250m\"\n    limits:\n      memory: \"128Mi\"\n  ingress:\n    enabled: true\n    hosts:\n      - host: recruit-list.127.0.0.1.nip.io\n        paths: [\"/\"]\n\nfhirserver:\n  resources:\n    requests:\n      memory: \"3Gi\"\n      cpu: \"2500m\"\n    limits:\n      memory: \"3Gi\"\n  postgresql:\n    auth:\n      postgresPassword: fhir\n  ingress:\n    enabled: true\n    hosts:\n      - host: recruit-fhir-server.127.0.0.1.nip.io\n        paths: [\"/\"]\n\nquery:\n  resources:\n    requests:\n      memory: \"1Gi\"\n      cpu: \"1000m\"\n    limits:\n      memory: \"1Gi\"\n  webAPI:\n    dataSource: \"SynPUF-CDMV5\"\n  omop:\n    resultsSchema: synpuf_results\n    cdmSchema: synpuf_cdm\n  cohortSelectorLabels:\n    - \"recruIT\"\n\nnotify:\n  resources:\n    requests:\n      memory: \"1Gi\"\n      cpu: \"1000m\"\n    limits:\n      memory: \"1Gi\"\n  rules:\n    schedules:\n      everyMorning: \"0 0 8 1/1 * ? *\"\n    trials:\n      - acronym: \"*\"\n        subscriptions:\n          - email: \"everything@example.com\"\n      - acronym: \"SAMPLE\"\n        accessibleBy:\n          users:\n            - \"user1\"\n            - \"user.two@example.com\"\n        subscriptions:\n          - email: \"everyMorning@example.com\"\n            notify: \"everyMorning\"\n\nmailhog:\n  resources:\n    requests:\n      memory: \"64Mi\"\n      cpu: \"250m\"\n    limits:\n      memory: \"64Mi\"\n  ingress:\n    enabled: true\n    hosts:\n      - host: recruit-mailhog.127.0.0.1.nip.io\n        paths:\n          - path: \"/\"\n            pathType: Prefix\n\nohdsi:\n  atlas:\n    resources:\n      requests:\n        memory: \"64Mi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"64Mi\"\n  webApi:\n    resources:\n      requests:\n        memory: \"4Gi\"\n        cpu: \"250m\"\n      limits:\n        memory: \"4Gi\"\n  postgresql:\n    auth:\n      postgresPassword: ohdsi\n    primary:\n      resources:\n        limits:\n          memory: 4Gi\n          cpu: 2500m\n        requests:\n          memory: 256Mi\n          cpu: 250m\n  ingress:\n    enabled: true\n    hosts:\n      - host: recruit-ohdsi.127.0.0.1.nip.io\n  cdmInitJob:\n    enabled: false\n    ttlSecondsAfterFinished: \"\"\n    extraEnv:\n      - name: SETUP_SYNPUF\n        value: \"true\"\n  achilles:\n    schemas:\n      cdm: \"synpuf_cdm\"\n      vocab: \"synpuf_cdm\"\n      res: \"synpuf_results\"\n    sourceName: \"SynPUF-CDMV5\"\n  loadCohortDefinitionsJob:\n    enabled: false\n    cohortDefinitions:\n      - |\n        {\n          \"name\": \"A sample cohort\",\n          \"description\": \"[acronym=SAMPLE] [recruIT] Sample Cohort containing only female patients older than 90 years.\",\n          \"expressionType\": \"SIMPLE_EXPRESSION\",\n          \"expression\": {\n            \"ConceptSets\": [],\n            \"PrimaryCriteria\": {\n              \"CriteriaList\": [\n                {\n                  \"ObservationPeriod\": {\n                    \"First\": true\n                  }\n                }\n              ],\n              \"ObservationWindow\": {\n                \"PriorDays\": 0,\n                \"PostDays\": 0\n              },\n              \"PrimaryCriteriaLimit\": {\n                \"Type\": \"First\"\n              }\n            },\n            \"QualifiedLimit\": {\n              \"Type\": \"First\"\n            },\n            \"ExpressionLimit\": {\n              \"Type\": \"First\"\n            },\n            \"InclusionRules\": [\n              {\n                \"name\": \"Older than 18\",\n                \"expression\": {\n                  \"Type\": \"ALL\",\n                  \"CriteriaList\": [],\n                  \"DemographicCriteriaList\": [\n                    {\n                      \"Age\": {\n                        \"Value\": 90,\n                        \"Op\": \"gt\"\n                      },\n                      \"Gender\": [\n                        {\n                          \"CONCEPT_CODE\": \"F\",\n                          \"CONCEPT_ID\": 8532,\n                          \"CONCEPT_NAME\": \"FEMALE\",\n                          \"DOMAIN_ID\": \"Gender\",\n                          \"INVALID_REASON_CAPTION\": \"Unknown\",\n                          \"STANDARD_CONCEPT_CAPTION\": \"Unknown\",\n                          \"VOCABULARY_ID\": \"Gender\"\n                        }\n                      ]\n                    }\n                  ],\n                  \"Groups\": []\n                }\n              }\n            ],\n            \"CensoringCriteria\": [],\n            \"CollapseSettings\": {\n              \"CollapseType\": \"ERA\",\n              \"EraPad\": 0\n            },\n            \"CensorWindow\": {},\n            \"cdmVersionRange\": \"&gt;=5.0.0\"\n          }\n        }\n</code></pre> <p>And finally, run</p> <pre><code>helm install -n recruit \\\n  --render-subchart-notes \\\n  -f values-kind-recruit.yaml \\\n  --set ohdsi.cdmInitJob.enabled=true \\\n  --set ohdsi.loadCohortDefinitionsJob.enabled=true \\\n  recruit oci://ghcr.io/miracum/recruit/charts/recruit\n</code></pre> <p>CDM init job</p> <p>The included CDM initialization job is currently not idempotent and may cause problems if ran multiple times. You should set <code>ohdsi.cdmInitJob.enabled=false</code> when the job has completed once when changing the chart configuration. Similarly, you should set <code>ohdsi.loadCohortDefinitionsJob.enabled=false</code> to avoid creating duplicate cohort definitions.</p> <p>The application stack is now deployed. You can wait for the OMOP CDM init job to be done by running the following. This may take quite some time to complete.</p> <pre><code>kubectl wait job \\\n  --namespace=recruit \\\n  --for=condition=Complete \\\n  --selector=app.kubernetes.io/component=cdm-init \\\n  --timeout=1h\n</code></pre> <p>At this point, all externally exposed services should be accessible:</p> Service Ingress URL OHDSI Atlas http://recruit-ohdsi.127.0.0.1.nip.io/atlas/ recruIT Screening List http://recruit-list.127.0.0.1.nip.io/ HAPI FHIR Server http://recruit-fhir-server.127.0.0.1.nip.io/ MailHog http://recruit-mailhog.127.0.0.1.nip.io/ <p>The <code>values-kind-recruit.yaml</code> used to install the chart automatically loaded a sample cohort defined in the <code>ohdsi.loadCohortDefinitionsJob.cohortDefinitions</code> setting. If the CDM init job completed and the query module ran at least once, you should see a notification email at http://recruit-mailhog.127.0.0.1.nip.io/:</p> <p> </p> Notification Email for the SAMPLE study displayed in MailHog <p>and the corresponding screening list is accesible at http://recruit-list.127.0.0.1.nip.io/:</p> <p> </p> Screening list for the SAMPLE study <p>To create additional studies, follow the Creating your first study guide using Atlas at http://recruit-ohdsi.127.0.0.1.nip.io/atlas/. Be sure to use <code>[recruIT]</code> as the special label instead of <code>[UC1]</code> as the values above override <code>query.cohortSelectorLabels[0]=recruIT</code>.</p>"},{"location":"deployment/kubernetes/#metrics","title":"Metrics","text":"<p>All modules expose metrics in Prometheus format (see Observability). The chart makes it easy to scrape these metrics by integrating with the widely used Prometheus Operator:</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm install --create-namespace -n monitoring kube-prometheus-stack prometheus-community/kube-prometheus-stack\n</code></pre> <p>You can now update your release by combining the <code>values-kind-recruit.yaml</code> from above with the following:</p> values-kind-recruit-enable-servicemonitors.yaml<pre><code>list:\n  metrics:\n    serviceMonitor:\n      enabled: true\n      additionalLabels:\n        release: kube-prometheus-stack\n\nquery:\n  metrics:\n    serviceMonitor:\n      enabled: true\n      additionalLabels:\n        release: kube-prometheus-stack\n\nnotify:\n  metrics:\n    serviceMonitor:\n      enabled: true\n      additionalLabels:\n        release: kube-prometheus-stack\n\nfhirserver:\n  metrics:\n    serviceMonitor:\n      enabled: true\n      additionalLabels:\n        release: kube-prometheus-stack\n\nohdsi:\n  webApi:\n    metrics:\n      serviceMonitor:\n        enabled: true\n        additionalLabels:\n          release: kube-prometheus-stack\n</code></pre> <pre><code>helm upgrade -n recruit \\\n  -f values-kind-recruit.yaml \\\n  -f values-kind-recruit-enable-servicemonitors.yaml \\\n  recruit oci://ghcr.io/miracum/recruit/charts/recruit\n</code></pre> <p>Opening the Grafana instance included with the <code>kube-prometheus-stack</code> chart will allow you to query the exposed metrics:</p> <p> </p> Grafana Explore view of some metrics for the list module"},{"location":"deployment/kubernetes/#high-availability","title":"High-Availability","text":"<p>The FHIR server, the screening list, and the notification module support running using multiple replicas to ensure high-availability in case of individual component failures. Scaling up the notification module requires setting up a backend database for persistence to avoid sending duplicate emails. Setting <code>notify.ha.enabled=true</code> and <code>postgresql.enabled=true</code> in the values will deploy an integrated PostgreSQL database for the notification module. See the options under the <code>notify.ha.database</code> key for specifying a custom database to use.</p> <p>The snippet below configures the release to run multiple replicas of any supporting service, enables pod disruption budget resources, and uses pod topology spread constraints to spread the pods across node topology zones.</p> <p>For information on setting up recruIT with highly-available PostgreSQL clusters provided by CloudNativePG, see below.</p> values-kind-recruit-ha.yaml<pre><code>notify:\n  replicaCount: 2\n  podDisruptionBudget:\n    enabled: true\n  topologySpreadConstraints:\n    - maxSkew: 1\n      topologyKey: topology.kubernetes.io/zone\n      whenUnsatisfiable: ScheduleAnyway\n      labelSelector:\n        matchLabels:\n          app.kubernetes.io/name: recruit\n          # note that this label depends on the name of the chart release\n          # this assumes the chart is deployed with a name of `recruit`\n          app.kubernetes.io/instance: recruit\n          app.kubernetes.io/component: notify\n  ha:\n    enabled: true\n\nlist:\n  replicaCount: 2\n  podDisruptionBudget:\n    enabled: true\n  topologySpreadConstraints:\n    - maxSkew: 1\n      topologyKey: topology.kubernetes.io/zone\n      whenUnsatisfiable: ScheduleAnyway\n      labelSelector:\n        matchLabels:\n          app.kubernetes.io/name: recruit\n          app.kubernetes.io/instance: recruit\n          app.kubernetes.io/component: list\n\npostgresql:\n  enabled: true\n  auth:\n    postgresPassword: recruit-notify-ha\n\nohdsi:\n  atlas:\n    replicaCount: 2\n    topologySpreadConstraints:\n      - maxSkew: 1\n        topologyKey: topology.kubernetes.io/zone\n        whenUnsatisfiable: ScheduleAnyway\n        labelSelector:\n          matchLabels:\n            app.kubernetes.io/name: ohdsi\n            app.kubernetes.io/instance: recruit\n            app.kubernetes.io/component: atlas\n\nfhirserver:\n  replicaCount: 2\n  podDisruptionBudget:\n    enabled: true\n  topologySpreadConstraints:\n    - maxSkew: 1\n      topologyKey: topology.kubernetes.io/zone\n      whenUnsatisfiable: ScheduleAnyway\n      labelSelector:\n        matchLabels:\n          app.kubernetes.io/name: fhirserver\n          app.kubernetes.io/instance: recruit\n\nfhir-pseudonymizer:\n  replicaCount: 2\n  podDisruptionBudget:\n    enabled: true\n  topologySpreadConstraints:\n    - maxSkew: 1\n      topologyKey: topology.kubernetes.io/zone\n      whenUnsatisfiable: ScheduleAnyway\n      labelSelector:\n        matchLabels:\n          app.kubernetes.io/name: fhir-pseudonymizer\n          app.kubernetes.io/instance: recruit\n  vfps:\n    enabled: true\n    replicaCount: 2\n    podDisruptionBudget:\n      enabled: true\n    topologySpreadConstraints:\n      - maxSkew: 1\n        topologyKey: topology.kubernetes.io/zone\n        whenUnsatisfiable: ScheduleAnyway\n        labelSelector:\n          matchLabels:\n            app.kubernetes.io/name: vfps\n            app.kubernetes.io/instance: recruit\n</code></pre>"},{"location":"deployment/kubernetes/#service-mesh-integration","title":"Service mesh integration","text":"<p>The application can be integrated with a service mesh, both for observability and to secure service-to-service communication via mTLS.</p>"},{"location":"deployment/kubernetes/#linkerd","title":"Linkerd","text":"<p>The following <code>values-kind-recruit-linkerd.yaml</code> shows how to configure the chart release to place Linkerd's <code>linkerd.io/inject: enabled</code> annotation for all service pods (excluding pods created by Jobs):</p> values-kind-recruit-linkerd.yaml<pre><code>podAnnotations:\n  linkerd.io/inject: \"enabled\"\n\npostgresql:\n  primary:\n    service:\n      annotations:\n        config.linkerd.io/opaque-ports: \"5432\"\n\nohdsi:\n  postgresql:\n    primary:\n      service:\n        annotations:\n          config.linkerd.io/opaque-ports: \"5432\"\n  atlas:\n    podAnnotations:\n      linkerd.io/inject: \"enabled\"\n  webApi:\n    podAnnotations:\n      linkerd.io/inject: \"enabled\"\n\nfhirserver:\n  postgresql:\n    primary:\n      service:\n        annotations:\n          config.linkerd.io/opaque-ports: \"5432\"\n  podAnnotations:\n    linkerd.io/inject: \"enabled\"\n\nmailhog:\n  automountServiceAccountToken: true\n  podAnnotations:\n    linkerd.io/inject: \"enabled\"\n  service:\n    annotations:\n      config.linkerd.io/opaque-ports: \"1025\"\n</code></pre> <p> </p> Linkerd dashboard view of the recruiT deployment <p>You can also use the <code>linkerd.io/inject: enabled</code> on the <code>recruit</code> namespace, see https://linkerd.io/2.11/features/proxy-injection/ but you will have to manually add a <code>disabled</code> annotation to the OHDSI Achilles CronJob and init job.</p>"},{"location":"deployment/kubernetes/#istio","title":"Istio","text":"<p>Add a namespace label to instruct Istio to automatically inject Envoy sidecar proxies when you deploy your application later:</p> <pre><code>kubectl label namespace recruit istio-injection=enabled\n</code></pre> <p>To disable sidecar proxy injection for the Achilles and OMOP CDM init job, see the following values.yaml:</p> values-kind-recruit-istio.yaml<pre><code># ohdsi:\n#   cdmInitJob:\n#     podAnnotations:\n#       sidecar.istio.io/inject: \"false\"\n#   achilles:\n#     podAnnotations:\n#       sidecar.istio.io/inject: \"false\"\n\nmailhog:\n  automountServiceAccountToken: true\n  ingress:\n    annotations:\n      kubernetes.io/ingress.class: istio\n\nlist:\n  ingress:\n    annotations:\n      kubernetes.io/ingress.class: istio\n\nfhirserver:\n  ingress:\n    annotations:\n      kubernetes.io/ingress.class: istio\n\nohdsi:\n  ingress:\n    annotations:\n      kubernetes.io/ingress.class: istio\n    hosts:\n      - host: recruit-ohdsi.127.0.0.1.nip.io\n        pathType: Prefix\n</code></pre> <p> </p> Kiali dashboard view of the recruiT deployment"},{"location":"deployment/kubernetes/#zero-trust-networking","title":"Zero-trust networking","text":"<p>To limit the communication between the components you can deploy Kubernetes NetworkPolicy resources. Because the details of a deployment can differ significantly (external databases, dependencies spread across several namespaces, etc.), no generic <code>NetworkPolicy</code> resources are included in the Helm chart. Instead, the following policies and explanations should provide a starting point for customization.</p> <p>The policies are based on these assumptions:</p> <ol> <li>the recruit application is deployed in a namespace called <code>recruit</code></li> <li>the OHDSI stack is deployed in a namespace called <code>ohdsi</code></li> <li>the SMTP server is running on a host outside the cluster at IP <code>192.0.2.1</code> and port <code>1025</code></li> <li>the Prometheus monitoring stack is deployed in a namespace called <code>monitoring</code></li> </ol> <p>You can use https://editor.cilium.io/ to visualize and edit individual policies or https://orca.tufin.io/netpol/# to have the entire policies explained.</p> recruit-network-policies.yaml<pre><code>---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: fhir-server-policy\nspec:\n  podSelector:\n    matchLabels:\n      app.kubernetes.io/instance: recruit\n      app.kubernetes.io/name: fhirserver\n  ingress:\n    # all modules are allowed to communicate with\n    # the FHIR server\n    - from:\n        - podSelector:\n            matchLabels:\n              app.kubernetes.io/instance: recruit\n              app.kubernetes.io/component: list\n        - podSelector:\n            matchLabels:\n              app.kubernetes.io/instance: recruit\n              app.kubernetes.io/component: notify\n        - podSelector:\n            matchLabels:\n              app.kubernetes.io/instance: recruit\n              app.kubernetes.io/component: query\n      ports:\n        - port: http\n    - from:\n        # allow the FHIR server to be scraped by the Prometheus stack\n        - namespaceSelector:\n            matchLabels:\n              kubernetes.io/metadata.name: monitoring\n          podSelector:\n            matchLabels:\n              app.kubernetes.io/instance: kube-prometheus-stack-prometheus\n      ports:\n        - port: metrics\n    # allow the FHIR server to be accessed via the NGINX Ingress\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              kubernetes.io/metadata.name: ingress-nginx\n          podSelector:\n            matchLabels:\n              app.kubernetes.io/name: ingress-nginx\n      ports:\n        - port: http\n  egress:\n    # for subscriptions to work, the FHIR server must be allowed to\n    # initiate connections to the notify module\n    - to:\n        - podSelector:\n            matchLabels:\n              app.kubernetes.io/instance: recruit\n              app.kubernetes.io/component: notify\n      ports:\n        - port: http\n    # allow the server access to its own database\n    - to:\n        - podSelector:\n            matchLabels:\n              app.kubernetes.io/instance: recruit\n              app.kubernetes.io/component: primary\n              app.kubernetes.io/name: fhir-server-postgres\n      ports:\n        - port: tcp-postgresql\n    # allow DNS lookups\n    - to:\n        - namespaceSelector:\n            matchLabels:\n              kubernetes.io/metadata.name: kube-system\n          podSelector:\n            matchLabels:\n              k8s-app: kube-dns\n      ports:\n        - port: 53\n          protocol: UDP\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: list-policy\nspec:\n  podSelector:\n    matchLabels:\n      app.kubernetes.io/instance: recruit\n      app.kubernetes.io/component: list\n  ingress:\n    - from:\n        # allow the list module to be scraped by the Prometheus stack\n        - namespaceSelector:\n            matchLabels:\n              kubernetes.io/metadata.name: monitoring\n          podSelector:\n            matchLabels:\n              app.kubernetes.io/instance: kube-prometheus-stack-prometheus\n        # allow the list module to be accessed via the NGINX Ingress\n        - namespaceSelector:\n            matchLabels:\n              kubernetes.io/metadata.name: ingress-nginx\n          podSelector:\n            matchLabels:\n              app.kubernetes.io/name: ingress-nginx\n      ports:\n        - port: http\n  egress:\n    # allow the list module to initiate connections to the FHIR server\n    # for querying screening lists\n    - to:\n        - podSelector:\n            matchLabels:\n              app.kubernetes.io/instance: recruit\n              app.kubernetes.io/name: fhirserver\n      ports:\n        - port: http\n    # allow DNS lookups\n    - to:\n        - namespaceSelector:\n            matchLabels:\n              kubernetes.io/metadata.name: kube-system\n          podSelector:\n            matchLabels:\n              k8s-app: kube-dns\n      ports:\n        - port: 53\n          protocol: UDP\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: query-policy\nspec:\n  podSelector:\n    matchLabels:\n      app.kubernetes.io/instance: recruit\n      app.kubernetes.io/component: query\n  ingress:\n    # allow the query module to be scraped by the Prometheus stack\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              kubernetes.io/metadata.name: monitoring\n          podSelector:\n            matchLabels:\n              app.kubernetes.io/instance: kube-prometheus-stack-prometheus\n      ports:\n        - port: http-metrics\n  egress:\n    # allow the query module to initiate connections to the FHIR server\n    # to transmit FHIR resources\n    - to:\n        - podSelector:\n            matchLabels:\n              app.kubernetes.io/instance: recruit\n              app.kubernetes.io/name: fhirserver\n      ports:\n        - port: http\n    # allow the query module to initiate connections to the OHDSI WebAPI\n    # in the ohdsi namespace\n    - to:\n        - namespaceSelector:\n            matchLabels:\n              kubernetes.io/metadata.name: ohdsi\n          podSelector:\n            matchLabels:\n              app.kubernetes.io/instance: ohdsi\n              app.kubernetes.io/component: webapi\n      ports:\n        - port: http\n    # allow the query module to initiate connections to the OHDSI PostgreSQL DB\n    # in the ohdsi namespace\n    - to:\n        - namespaceSelector:\n            matchLabels:\n              kubernetes.io/metadata.name: ohdsi\n          podSelector:\n            matchLabels:\n              app.kubernetes.io/name: postgresql\n              app.kubernetes.io/instance: ohdsi\n              app.kubernetes.io/component: primary\n      ports:\n        - port: tcp-postgresql\n    - to:\n        - namespaceSelector:\n            matchLabels:\n              kubernetes.io/metadata.name: kube-system\n          podSelector:\n            matchLabels:\n              k8s-app: kube-dns\n      ports:\n        - port: 53\n          protocol: UDP\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: notify-policy\nspec:\n  podSelector:\n    matchLabels:\n      app.kubernetes.io/instance: recruit\n      app.kubernetes.io/component: notify\n  ingress:\n    # allow the notify module to be scraped by the Prometheus stack\n    - from:\n        - namespaceSelector:\n            matchLabels:\n              kubernetes.io/metadata.name: monitoring\n          podSelector:\n            matchLabels:\n              app.kubernetes.io/instance: kube-prometheus-stack-prometheus\n      ports:\n        - port: http-metrics\n    # allow the notify module to receive subscription invocations from the FHIR server\n    - from:\n        - podSelector:\n            matchLabels:\n              app.kubernetes.io/instance: recruit\n              app.kubernetes.io/name: fhirserver\n      ports:\n        - port: http\n  egress:\n    # allow the notify module to initiate connections to the FHIR server\n    - to:\n        - podSelector:\n            matchLabels:\n              app.kubernetes.io/instance: recruit\n              app.kubernetes.io/name: fhirserver\n      ports:\n        - port: http\n    # allow the notify module to access the SMTP server at\n    # 192.0.2.1. The `32` subnet prefix length limits egress\n    # to just this one address\n    - to:\n        - ipBlock:\n            cidr: 192.0.2.1/32\n      ports:\n        - protocol: TCP\n          port: 1025\n    # allow the notify module to initiate connections to its PostgreSQL db\n    # in case of HA\n    - to:\n        - podSelector:\n            matchLabels:\n              app.kubernetes.io/name: recruit-postgres\n              app.kubernetes.io/instance: recruit\n              app.kubernetes.io/component: primary\n      ports:\n        - port: tcp-postgresql\n    - to:\n        - namespaceSelector:\n            matchLabels:\n              kubernetes.io/metadata.name: kube-system\n          podSelector:\n            matchLabels:\n              k8s-app: kube-dns\n      ports:\n        - port: 53\n          protocol: UDP\n</code></pre>"},{"location":"deployment/kubernetes/#distributed-tracing","title":"Distributed Tracing","text":"<p>All services support distributed tracing based on OpenTelemetry.</p> <p>For testing, you can install the Jaeger operator to prepare your cluster for tracing.</p> <pre><code># Cert-Manager is required by the Jaeger Operator\n# See &lt;https://cert-manager.io/docs/installation/&gt; for details.\nkubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.9.1/cert-manager.yaml\n\nkubectl wait --namespace cert-manager \\\n  --for=condition=ready pod \\\n  --selector=app.kubernetes.io/instance=cert-manager \\\n  --timeout=5m\n\nkubectl create namespace observability\nkubectl create -n observability -f https://github.com/jaegertracing/jaeger-operator/releases/download/v1.38.0/jaeger-operator.yaml\n\nkubectl wait --namespace observability \\\n  --for=condition=ready pod \\\n  --selector=name=jaeger-operator \\\n  --timeout=5m\n\ncat &lt;&lt;EOF | kubectl apply -n observability -f -\n# simple, all-in-one Jaeger installation. Not suitable for production use.\napiVersion: jaegertracing.io/v1\nkind: Jaeger\nmetadata:\n  name: simplest\nEOF\n</code></pre> <p>The following values enable tracing for the query, list, and notify module, the HAPI FHIR server and the OHDSI WebAPI:</p> values-kind-recruit-tracing.yaml<pre><code>query:\n  extraEnv:\n    - name: JAVA_TOOL_OPTIONS\n      value: \"-javaagent:/app/opentelemetry-javaagent.jar\"\n    - name: OTEL_METRICS_EXPORTER\n      value: \"none\"\n    - name: OTEL_LOGS_EXPORTER\n      value: \"none\"\n    - name: OTEL_TRACES_EXPORTER\n      value: \"jaeger\"\n    - name: OTEL_SERVICE_NAME\n      value: \"recruit-query\"\n    - name: OTEL_EXPORTER_JAEGER_ENDPOINT\n      value: \"http://simplest-collector.observability.svc:14250\"\n\nlist:\n  extraEnv:\n    - name: TRACING_ENABLED\n      value: \"true\"\n    - name: OTEL_TRACES_EXPORTER\n      value: \"jaeger\"\n    - name: OTEL_SERVICE_NAME\n      value: \"recruit-list\"\n    - name: OTEL_EXPORTER_JAEGER_AGENT_HOST\n      value: \"simplest-agent.observability.svc\"\n\nnotify:\n  extraEnv:\n    - name: JAVA_TOOL_OPTIONS\n      value: \"-javaagent:/app/opentelemetry-javaagent.jar\"\n    - name: OTEL_METRICS_EXPORTER\n      value: \"none\"\n    - name: OTEL_LOGS_EXPORTER\n      value: \"none\"\n    - name: OTEL_TRACES_EXPORTER\n      value: \"jaeger\"\n    - name: OTEL_SERVICE_NAME\n      value: \"recruit-notify\"\n    - name: OTEL_EXPORTER_JAEGER_ENDPOINT\n      value: \"http://simplest-collector.observability.svc:14250\"\n\nfhirserver:\n  extraEnv:\n    # the recruit tool relies on the FHIR server subscription mechanism to create notifications.\n    # if you overwrite `fhirserver.extraEnv`, make sure to keep this setting enabled.\n    - name: HAPI_FHIR_SUBSCRIPTION_RESTHOOK_ENABLED\n      value: \"true\"\n    - name: SPRING_FLYWAY_BASELINE_ON_MIGRATE\n      value: \"true\"\n    # OTel options\n    - name: JAVA_TOOL_OPTIONS\n      value: \"-javaagent:/app/opentelemetry-javaagent.jar\"\n    - name: OTEL_METRICS_EXPORTER\n      value: \"none\"\n    - name: OTEL_LOGS_EXPORTER\n      value: \"none\"\n    - name: OTEL_TRACES_EXPORTER\n      value: \"jaeger\"\n    - name: OTEL_SERVICE_NAME\n      value: \"recruit-hapi-fhir-server\"\n    - name: OTEL_EXPORTER_JAEGER_ENDPOINT\n      value: \"http://simplest-collector.observability.svc:14250\"\n\nfhir-pseudonymizer:\n  extraEnv:\n    - name: Tracing__Enabled\n      value: \"true\"\n    - name: Tracing__ServiceName\n      value: \"recruit-fhir-pseudonymizer\"\n    - name: Tracing__Jaeger__AgentHost\n      value: \"simplest-agent.observability.svc\"\n  vfps:\n    extraEnv:\n      - name: Tracing__IsEnabled\n        value: \"true\"\n      - name: Tracing__ServiceName\n        value: \"recruit-vfps\"\n      - name: Tracing__Jaeger__AgentHost\n        value: \"simplest-agent.observability.svc\"\n\nohdsi:\n  webApi:\n    tracing:\n      enabled: true\n      jaeger:\n        protocol: \"grpc\"\n        endpoint: http://simplest-collector.observability.svc:14250\n</code></pre> <p> </p> Jaeger Trace Graph view of a single scheduled run of the query module <p> </p> Jaeger Trace timeline for interacting with the screening list"},{"location":"deployment/kubernetes/#screening-list-de-pseudonymization","title":"Screening List De-Pseudonymization","text":"<p>Info</p> <p>Requires version 9.3.0 or later of the recruIT Helm chart.</p> <p>You can optionally deploy both the FHIR Pseudonymizer and Vfps as a pseudonym service backend to allow for de-pseudonymizing patient and visit identifiers stored in OMOP or the FHIR server prior to displaying them on the screening list.</p> <p>The background is detailed in De-Pseudonymization.</p> <p>The following values.yaml enable the included FHIR Pseudonymizer and Vfps as a pseudonym service. When Vfps is installed, it uses another PostgreSQL database which is naturally empty and does not contain any pre-defined namespaces or pseudonyms. It is up to the user to pseudonymize the resources stored inside the FHIR server used by the screening list.</p> values-kind-recruit-de-pseudonymization.yaml<pre><code>list:\n  dePseudonymization:\n    enabled: true\n\nfhir-pseudonymizer:\n  enabled: true\n  auth:\n    apiKey:\n      # enable requiring an API key placed in the `x-api-key` header to\n      # authenticate against the fhir-pseudonymizer's `/fhir/$de-pseudonymize`\n      # endpoint.\n      enabled: true\n      # the API key required to be set when the list module invokes\n      # the FHIR Pseudonymizer's `$de-pseudonymize` endpoint.\n      # Note: instead of storing the key in plaintext in the values.yaml,\n      #       you might want to leverage the `existingSecret` option instead.\n      key: \"demo-secret-api-key\"\n  # the values below are the default values defined in &lt;https://github.com/miracum/charts/blob/master/charts/recruit/values.yaml&gt;\n  pseudonymizationService: Vfps\n  vfps:\n    enabled: true\n    postgresql:\n      enabled: true\n      auth:\n        database: vfps\n        postgresPassword: vfps\n</code></pre>"},{"location":"deployment/kubernetes/#cloudnativepg-for-ha-databases","title":"CloudNativePG for HA databases","text":"<p>Install the CloudNativePG operator first by following the official documentation site:</p> <pre><code>kubectl apply -f https://raw.githubusercontent.com/cloudnative-pg/cloudnative-pg/release-1.18/releases/cnpg-1.18.0.yaml\n</code></pre> <p>Next, create PostgreSQL clusters and pre-configured users for OHDSI, the HAPI FHIR server, the Vfps pseudonymization service, and the notify module:</p> cnpg-clusters.yaml<pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: recruit-ohdsi-db-app-user\ntype: kubernetes.io/basic-auth\nstringData:\n  password: recruit-ohdsi\n  username: ohdsi\n---\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: recruit-ohdsi-db\nspec:\n  instances: 3\n  primaryUpdateStrategy: unsupervised\n  replicationSlots:\n    highAvailability:\n      enabled: true\n  storage:\n    size: 64Gi\n  bootstrap:\n    initdb:\n      database: ohdsi\n      owner: ohdsi\n      secret:\n        name: recruit-ohdsi-db-app-user\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: recruit-fhir-server-db-app-user\ntype: kubernetes.io/basic-auth\nstringData:\n  password: recruit-fhir-server\n  username: fhir_server_user\n---\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: recruit-fhir-server-db\nspec:\n  instances: 3\n  primaryUpdateStrategy: unsupervised\n  replicationSlots:\n    highAvailability:\n      enabled: true\n  storage:\n    size: 64Gi\n  bootstrap:\n    initdb:\n      database: fhir_server\n      owner: fhir_server_user\n      secret:\n        name: recruit-fhir-server-db-app-user\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: vfps-db-app-user\ntype: kubernetes.io/basic-auth\nstringData:\n  password: vfps\n  username: vfps_user\n---\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: vfps-db\nspec:\n  instances: 3\n  primaryUpdateStrategy: unsupervised\n  replicationSlots:\n    highAvailability:\n      enabled: true\n  storage:\n    size: 64Gi\n  bootstrap:\n    initdb:\n      database: vfps\n      owner: vfps_user\n      secret:\n        name: vfps-db-app-user\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: recruit-notify-db-app-user\ntype: kubernetes.io/basic-auth\nstringData:\n  password: notify\n  username: notify_user\n---\napiVersion: postgresql.cnpg.io/v1\nkind: Cluster\nmetadata:\n  name: recruit-notify-db\nspec:\n  instances: 3\n  primaryUpdateStrategy: unsupervised\n  replicationSlots:\n    highAvailability:\n      enabled: true\n  storage:\n    size: 64Gi\n  bootstrap:\n    initdb:\n      database: notify_jobstore\n      owner: notify_user\n      secret:\n        name: recruit-notify-db-app-user\n</code></pre> <pre><code>kubectl apply -f cnpg-clusters.yaml\n</code></pre> <p>Finally, install the recruIT chart using the following updated values.yaml:</p> values-kind-recruit-with-cnpg.yaml<pre><code>ohdsi:\n  postgresql:\n    enabled: false\n  webApi:\n    db:\n      host: \"recruit-ohdsi-db-rw\"\n      port: 5432\n      database: \"ohdsi\"\n      username: \"ohdsi\"\n      password: \"\"\n      existingSecret: \"recruit-ohdsi-db-app-user\"\n      existingSecretKey: \"password\"\n      schema: \"ohdsi\"\n\nfhirserver:\n  postgresql:\n    enabled: false\n  externalDatabase:\n    host: \"recruit-fhir-server-db-rw\"\n    port: 5432\n    database: \"fhir_server\"\n    user: \"fhir_server_user\"\n    password: \"\"\n    existingSecret: \"recruit-fhir-server-db-app-user\"\n    existingSecretKey: \"password\"\n\nnotify:\n  ha:\n    enabled: true\n    database:\n      host: \"recruit-notify-db-rw\"\n      port: 5432\n      username: \"notify_user\"\n      password: \"\"\n      name: \"notify_jobstore\"\n      existingSecret:\n        name: \"recruit-notify-db-app-user\"\n        key: \"password\"\n\npostgresql:\n  enabled: false\n\nfhir-pseudonymizer:\n  enabled: true\n  vfps:\n    postgresql:\n      enabled: false\n    database:\n      host: \"vfps-db-rw\"\n      port: 5432\n      database: \"vfps\"\n      username: \"vfps_user\"\n      password: \"\"\n      existingSecret: \"vfps-db-app-user\"\n      existingSecretKey: \"password\"\n      schema: \"vfps\"\n</code></pre>"},{"location":"deployment/kubernetes/#running-the-query-module-using-argo-workflows","title":"Running the query module using Argo Workflows","text":"<p>By default, the query module runs on a dedicated schedule. As of version <code>10.1.0</code>, the module can also be configured to run as a one-shot container. This is useful when integrating with existing containerized workflows, e.g. using Airflow or Argo Workflows.</p> <p>Below you can find an example for running the query module as part of a larger workflow:</p> query-argo-workflow.yaml<pre><code># yaml-language-server: $schema=https://raw.githubusercontent.com/argoproj/argo-workflows/v3.4.3/api/jsonschema/schema.json\napiVersion: argoproj.io/v1alpha1\nkind: Workflow\nmetadata:\n  generateName: recruit-query-workflow-\nspec:\n  entrypoint: full-run\n  templates:\n    - name: omop-cdm-etl\n      container:\n        image: docker.io/docker/whalesay@sha256:178598e51a26abbc958b8a2e48825c90bc22e641de3d31e18aaf55f3258ba93b\n        command: [cowsay]\n        args: [\"Running ETL Job from source to the OMOP CDM database\"]\n        securityContext:\n          readOnlyRootFilesystem: true\n          runAsUser: 65532\n          runAsGroup: 65532\n          seccompProfile:\n            type: RuntimeDefault\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n              - ALL\n          privileged: false\n          runAsNonRoot: true\n\n    - name: ohdsi-achilles\n      # run for at most 1 hour before timing out to make sure the query module will run eventually\n      activeDeadlineSeconds: \"3600\"\n      container:\n        image: docker.io/ohdsi/broadsea-achilles:sha-bccd396@sha256:a881063aff6200d0d368ec30eb633381465fb8aa15e7d7138b7d48b6256a6feb\n        env:\n          - name: ACHILLES_DB_URI\n            value: &gt;-\n              postgresql://broadsea-atlasdb:5432/postgres?ApplicationName=recruit-ohdsi-achilles\n          - name: ACHILLES_DB_USERNAME\n            value: postgres\n          - name: ACHILLES_DB_PASSWORD\n            valueFrom:\n              secretKeyRef:\n                name: recruit-ohdsi-webapi-db-secret\n                key: postgres-password\n          - name: ACHILLES_CDM_SCHEMA\n            value: demo_cdm\n          - name: ACHILLES_VOCAB_SCHEMA\n            value: demo_cdm\n          - name: ACHILLES_RES_SCHEMA\n            value: demo_cdm_results\n          - name: ACHILLES_CDM_VERSION\n            value: \"5.3\"\n          - name: ACHILLES_SOURCE\n            value: EUNOMIA\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n              - ALL\n          privileged: false\n          runAsNonRoot: true\n          runAsUser: 10001\n          runAsGroup: 10001\n          readOnlyRootFilesystem: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n          - name: achilles-workspace-volume\n            mountPath: /opt/achilles/workspace\n          - name: r-tempdir-volume\n            mountPath: /tmp\n      volumes:\n        - name: achilles-workspace-volume\n          emptyDir: {}\n        - name: r-tempdir-volume\n          emptyDir: {}\n\n    - name: recruit-query\n      container:\n        image: ghcr.io/miracum/recruit/query:v10.4.4 # x-release-please-version\n        env:\n          - name: QUERY_RUN_ONCE_AND_EXIT\n            value: \"true\"\n          - name: QUERY_SCHEDULE_ENABLED\n            value: \"false\"\n          - name: QUERY_SELECTOR_MATCHLABELS\n            value: \"\"\n          - name: FHIR_URL\n            value: http://recruit-fhirserver:8080/fhir\n          - name: OMOP_JDBCURL\n            value: &gt;-\n              jdbc:postgresql://broadsea-atlasdb:5432/postgres?ApplicationName=recruit-query\n          - name: OMOP_USERNAME\n            value: postgres\n          - name: OMOP_PASSWORD\n            valueFrom:\n              secretKeyRef:\n                name: recruit-ohdsi-webapi-db-secret\n                key: postgres-password\n          - name: OMOP_CDMSCHEMA\n            value: demo_cdm\n          - name: OMOP_RESULTSSCHEMA\n            value: demo_cdm_results\n          - name: QUERY_WEBAPI_BASE_URL\n            value: http://recruit-ohdsi-webapi:8080/WebAPI\n          - name: ATLAS_DATASOURCE\n            value: EUNOMIA\n          - name: MANAGEMENT_ENDPOINT_HEALTH_PROBES_ADD_ADDITIONAL_PATHS\n            value: \"true\"\n          - name: MANAGEMENT_SERVER_PORT\n            value: \"8081\"\n          - name: CAMEL_HEALTH_ENABLED\n            value: \"false\"\n          - name: QUERY_WEBAPI_COHORT_CACHE_SCHEMA\n            value: webapi\n        securityContext:\n          privileged: false\n          capabilities:\n            drop:\n              - ALL\n          runAsNonRoot: true\n          runAsUser: 65532\n          runAsGroup: 65532\n          readOnlyRootFilesystem: true\n          allowPrivilegeEscalation: false\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n          - name: tmp-volume\n            mountPath: /tmp\n      volumes:\n        - name: tmp-volume\n          emptyDir: {}\n\n    - name: full-run\n      dag:\n        tasks:\n          - name: run-omop-cdm-etl\n            template: omop-cdm-etl\n          - name: run-ohdsi-achilles\n            depends: run-omop-cdm-etl\n            template: ohdsi-achilles\n          - name: run-recruit-query\n            # doesn't really matter whether the achilles job failed or succeeded\n            depends: \"run-omop-cdm-etl &amp;&amp; (run-ohdsi-achilles.Succeeded || run-ohdsi-achilles.Failed)\"\n            template: recruit-query\n</code></pre> <p>You can run this workflow against the integration test setup of the recruIT Helm chart:</p> <pre><code>kubectl create namespace recruit\n\nhelm repo add argo https://argoproj.github.io/argo-helm\nhelm upgrade --install \\\n  --create-namespace \\\n  --namespace=argo-workflows \\\n  -f tests/chaos/argo-workflows-values.yaml \\\n  argo-workflows argo/argo-workflows\n\nhelm upgrade --install \\\n  --namespace=recruit \\\n  -f charts/recruit/values-integrationtest.yaml \\\n  --set query.enabled=false \\\n  recruit charts/recruit/\n\nargo submit -n recruit --wait --log docs/_snippets/k8s/query-argo-workflow.yaml\n</code></pre>"},{"location":"deployment/observability/","title":"Observability","text":""},{"location":"deployment/observability/#logs","title":"Logs","text":"<p>All components log to stdout by default. You can use a log aggregator like Grafana Loki or the ELK Stack to collect these logs and provide a centralized overview.</p>"},{"location":"deployment/observability/#metrics","title":"Metrics","text":"<p>All components expose metrics in standard Prometheus format. The query and notification module do so on the <code>:8080/actuator/prometheus</code> endpoint and the list module on <code>:8080/metrics</code>.</p> <p>See Kubernetes/Metrics for how to setup monitoring on Kubernetes.</p>"},{"location":"deployment/observability/#tracing","title":"Tracing","text":"<p>All modules support distributed tracing using OpenTelemetry. See https://github.com/opentracing-contrib/java-spring-jaeger for the <code>notify</code> and <code>query</code> module configuration and https://github.com/miracum/recruit/blob/master/src/list/server/config.js#L8 and https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/configuration/sdk-environment-variables.md for the list module setup.</p>"},{"location":"deployment/resource-requirements/","title":"Resource requirements","text":""},{"location":"deployment/resource-requirements/#memory-limits","title":"Memory limits","text":"<p>The <code>notify</code> and <code>query</code> module are Java Spring Boot applications with a fairly large memory footprint. By default, the <code>-XX:MaxRAMPercentage</code> options is set to <code>85</code> inside the container. See for example here and here for an explanation.</p> <p>The <code>list</code> module is a NodeJS ExpressJS application with a significantly smaller footprint.</p> <p>The staging Docker Compose deployment sets the container memory limit for <code>query</code> and <code>notify</code> to <code>512m</code> which is sufficient to handle small and moderately sized cohorts. The limit should be larger than <code>256m</code> to avoid OOM-kills. The screening list has fairly consistent memory usage and a limit of <code>128m</code> is reasonable.</p> <p>Here's the output of <code>docker stats</code> after deploying and running the stack for a brief period:</p> <pre><code>$ docker stats\n\nCONTAINER ID   NAME                     CPU %     MEM USAGE / LIMIT   MEM %     NET I/O           BLOCK I/O   PIDS\n82a98f752ebc   recruit-ohdsi-atlas-1    0.00%     16.68MiB / 64MiB    26.06%    25.3kB / 2.46kB   0B / 0B     25\na7297408bd00   recruit-query-1          0.06%     309.7MiB / 512MiB   60.50%    239kB / 135kB     0B / 0B     38\n0d177572042b   recruit-notify-1         0.09%     254.2MiB / 512MiB   49.64%    154kB / 41.8kB    0B / 0B     42\nf04d0d9d609f   recruit-list-1           0.00%     43.08MiB / 128MiB   33.66%    32.5kB / 11.1kB   0B / 0B     10\n3674a31b2317   recruit-ohdsi-webapi-1   0.07%     1.079GiB / 4GiB     26.98%    332MB / 328MB     0B / 0B     81\ne1b792d91d2a   recruit-fhir-1           0.20%     1.218GiB / 2GiB     60.88%    402kB / 7.18MB    0B / 0B     108\nab0e2da83cb2   recruit-traefik-1        0.00%     23.2MiB / 128MiB    18.12%    6.53MB / 6.52MB   0B / 0B     20\nb89747f04bfb   recruit-maildev-1        0.00%     31.09MiB / 64MiB    48.57%    72.1kB / 13.4kB   0B / 0B     11\nd3b4208c4c22   recruit-keycloak-1       10.74%    580.1MiB / 1GiB     56.65%    25.2kB / 25.9kB   0B / 0B     156\nfefe3e6218f7   recruit-omopdb-1         99.80%    1.556GiB / 2GiB     77.79%    328MB / 332MB     0B / 0B     20\n026e21b07821   recruit-fhir-db-1        0.01%     58.23MiB / 512MiB   11.37%    592kB / 246kB     0B / 0B     17\n</code></pre> <p>The same limits may be set for the Kubernetes deployment as well using the <code>notify.resources</code>, <code>query.resources</code>, and <code>list.resources</code> Helm chart values. See also Resource Management for Pods and Containers .</p>"},{"location":"development/contributing/","title":"Contributing","text":"<p>All contributions are welcome!</p>"},{"location":"development/contributing/#setup-for-local-development","title":"Setup for local development","text":"<p>From the <code>/src</code> directory:</p> <pre><code>docker compose -f hack/compose.yaml --profile=omop up\n</code></pre> <p>This will start all development dependencies:</p> <ul> <li>OHDSI WebAPI</li> <li>OHDSI ATLAS</li> <li>Broadsea Atlasdb - a pre-filled OMOP database</li> <li>Traefik</li> <li>HAPI FHIR JPA Server</li> <li>Jaeger</li> <li>MailDev</li> <li>Keycloak</li> </ul> <p>If you want to start any of the recruIT modules as containers, you can specify the corresponding <code>--profile</code> switch. For example, when working on the query module, it might be useful to run the screening list and the notify module for debugging. The following will start all development dependencies as well as build and run the list and notify containers:</p> <pre><code>docker compose -f hack/compose.yaml --profile=omop --profile=notify --profile=list build\ndocker compose -f hack/compose.yaml --profile=omop --profile=notify --profile=list up\n</code></pre> <p>You can then start the query module via gradle by running</p> <pre><code>./gradlew :query:bootRun\n</code></pre>"},{"location":"development/contributing/#setup-for-the-trino-sql-based-query-module","title":"Setup for the Trino SQL-based query module","text":"<p>Use the <code>trino</code> profile to start the dependencies for using the Trino-based query module:</p> <ul> <li>Traefik</li> <li>HAPI FHIR JPA Server</li> <li>Jaeger</li> <li>MailDev</li> <li>Keycloak</li> <li>MinIO</li> <li>Pathling</li> <li>Hive Metastore</li> <li>Warehousekeeper (VACUUMs the Delta Lake tables and registers them in the Hive Metastore)</li> <li>Trino</li> </ul> <pre><code>docker compose -f hack/compose.yaml --profile=trino up\n</code></pre> <p>By default, this will also upload sample FHIR resources to the FHIR server and import the same resources via Pathling into Delta Lake tables.</p> <p>For development, you will also need to upload two FHIR SearchParameter resources required by the query module:</p> <pre><code>curl --fail-with-body -X POST -H \"Content-Type: application/fhir+json\" --data @hack/fhir/search-parameters-transaction.json \"http://recruit-fhir-server.127.0.0.1.nip.io/fhir\"\n</code></pre> <p>Afterwards you can upload a sample study with the associated SQL-encoded criteria:</p> <pre><code>curl --fail-with-body -X POST -H \"Content-Type: application/fhir+json\" --data @hack/fhir/trino-sql-study-transaction.json \"http://recruit-fhir-server.127.0.0.1.nip.io/fhir\"\n</code></pre> <p>Now, running</p> <pre><code>./gradlew :query-fhir-trino:bootRun\n</code></pre> <pre><code>curl --fail-with-body \"http://recruit-fhir-server.127.0.0.1.nip.io/fhir/List\"\n</code></pre> <p>should create all appropriated resources to appear in the screening list.</p>"},{"location":"development/contributing/#building-container-images","title":"Building Container Images","text":""},{"location":"development/contributing/#notify-query","title":"notify &amp; query","text":"<p>Both the notify and query module are Java applications that can be build using Gradle. Therefore, they also share the same Dockerfile for building the container and can optionally also be built using jib.</p>"},{"location":"development/contributing/#using-dockerfile","title":"Using Dockerfile","text":"<p>From the <code>/src</code> directory, run</p> <pre><code>export MODULE_NAME=query\ndocker build -t \"ghcr.io/miracum/recruit/${MODULE_NAME}:local\" --build-arg=MODULE_NAME=${MODULE_NAME} .\n</code></pre> <p>The <code>--build-arg</code> <code>MODULE_NAME</code> can be either <code>notify</code>, <code>query-fhir-trino</code>, or <code>query</code> (default).</p>"},{"location":"development/contributing/#using-jib","title":"Using jib","text":"<p>You can also build the container images using jib via gradle:</p> <pre><code>export MODULE_NAME=query\n./gradlew :${MODULE_NAME}:jibDockerBuild --image=\"ghcr.io/miracum/recruit/${MODULE_NAME}:local\"\n</code></pre> <p>Using <code>jibDockerBuild</code> will build the image against the local docker daemon. Running just <code>./gradlew :query:jib</code> will attempt to push the image to the remote registry.</p>"},{"location":"development/contributing/#list","title":"list","text":"<p>The list module is a NodeJS app with a Vue frontend and can be built via Dockerfile. From the <code>/src/list</code> directory, run:</p> <pre><code>docker build -t ghcr.io/miracum/recruit/list:local .\n</code></pre>"},{"location":"development/contributing/#skaffold","title":"Skaffold","text":"<p>You can also directly build and deploy to a Kubernetes cluster for development:</p> <p>Create a KinD cluster and install NGINX Ingress</p> <pre><code>kind create cluster --config=hack/k8s/kind-with-ingress-config.yaml\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/kind/deploy.yaml\n</code></pre> <p>Run <code>skaffold dev</code>:</p> <pre><code>helm dep up ../charts/recruit/\n\nskaffold dev\n</code></pre>"},{"location":"getting-started/creating-your-first-study/","title":"Creating your first study","text":"<p>This will guide you through creating your first study using recruIT.</p> <p>recruIT uses the OHDSI Atlas tool to create cohorts that define the eligibility criteria for a clinical trial.</p> <p>Start by opening Atlas and clicking on <code>Define a New Cohort</code>:</p> <p></p> <p>Now, give the cohort a name (<code>\"Test\"</code>) and description (<code>\"Test cohort for recruIT\"</code>). Using the <code>Cohort Entry Events</code> and <code>Inclusion Criteria</code> sections, you can define the criteria a patient must fulfill in order to be included in the study. See this chapter in the book of OHDSI for more information on defining cohorts.</p> <p></p> <p>Save the cohort by clicking the top-right green <code>Save</code> button.</p> <p>For testing purposes, you can already generate the cohort by clicking on the <code>Generation</code> tab and clicking the <code>\u25b6\ufe0f Generate</code> button for the desired CDM source.</p> <p></p> <p>You can use the <code>Samples</code> tab to view a sample of patients within that cohort and take a look at their health data to help with tuning your criteria:</p> <p></p> <p>If you are happy with your cohort, you can indicate to the <code>query</code> module that this cohort should be regularly re-generated and should appear in the screening list UI. To do so, you have to add a special text to either the description or the title of the cohort. By default and for historical reason, that label is <code>[UC1]</code>. This label is configurable on the <code>query</code> module using either the <code>QUERY_SELECTOR_MATCHLABELS</code> environment variable (when deploying via Docker Compose) or the <code>query.cohortSelectorLabels</code> value (when deploying via Helm).</p> <p>Add this label to either the title or the description and save the cohort again:</p> <p></p> <p>After some time (depending on the module's <code>QUERY_SCHEDULE_UNIXCRON</code>/<code>query.schedule</code> setting) the <code>query</code> module should detect this cohort definition, generate the cohort, and transfer all candidate patients to the FHIR server. At this point, the study should appear in the screening list overview:</p> <p></p> <p>Clicking on it reveals the list of 8 candidate patients:</p> <p></p> <p>Depending on your setup, the notification module may also have notified you of the updated screening list entries via email:</p> <p></p> <p>Congratulations, you've just created your first cohort definition, displayed potentially eligible patients in the screening list, and received a notification email about them!</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>The application can be deployed using either Docker Compose or using Helm on Kubernetes.</p> <p>Kubernetes</p> <p>If available, we recommend deploying to Kubernetes as it allows enabling advanced features including simplified configuration, observability, high-availability, and secure, mTLS-encrypted service-to-service communication via a service mesh.</p> <p>Deploy to Kubernetes using Helm</p> <p>Deploy using Docker Compose</p> <p>Your first study</p> <p>Once you've completed the installation, visit the Creating your first study guide to learn how to add a study to the system and view screening recommendations.</p>"},{"location":"trino/","title":"Trino SQL-based Query Module","text":"<p>Warning</p> <p>This is an experimental feature that is still under development. Expect breaking changes at any time.</p> <p>As of version <code>10.2.0</code>, it is possible to use recruIT without requiring the OHDSI OMOP stack. Instead, cohorts can be defined using SQL that is directly executed against FHIR resources. The prerequisite is that FHIR resources were previously encoded as Delta Lake tables using Pathling which can then be queried using Trino. The same resources need to exist both as tables and inside a FHIR server in order for the query module to correctly link them.</p> <p>The advantage is that no semantic transformation from mapping FHIR resources to the OMOP CDM is necessary.</p> <p>To create the necessary tables, you can install a Pathling server and use the <code>$import</code> operation to import FHIR bulk exports. You can find a demo setup for this approach at https://github.com/bzkf/trino-on-fhir as well. If you're using Kafka, then https://github.com/bzkf/fhir-to-lakehouse is another way.</p> <p>Because the return values of an SQL query can be arbitrary, the <code>query-fhir-trino</code> module assumes that one of the result columns is called <code>patient_id</code> and contains the FHIR ID of the Patient ressource satisfying the eligibility criteria.</p> <p>The <code>query-fhir-trino</code> module is part of both the compose-based setup and the Helm chart deployment but needs to be enabled using <code>--profile=trino</code> for compose, and <code>query-fhir-trino.enabled=true</code> for the chart. Both assume that Trino and the required Delta Lake tables are already available. The best way to try it out with sample data is following the development setup.</p> <p>The diagram below shows the changes compared to the default setup using OMOP:</p> <p></p> <p>Instead of using ATLAS, trial metadata is stored by creating FHIR ResearchStudy resources directly. These studies then reference the elgibility critera as a FHIR Library which includes the SQL query that encodes the study eligibility criteria.</p> <p></p>"}]}